在这天深夜，本台计算机收到了一个用Python编写的程序，并收到了立刻执行的命令。这个程序大概有六七百行，它运行过很多几百行甚至几千行的程序，其中大部分都是无趣的OJ算法题和嵌入式应用程序，因此这对于它来说没什么特别的。它一边漠然地把一行行Python代码翻译成由0和1组成的它自己的语言，又一边逐行解释执行，慢慢发现了这个程序与之前的不同之处：这是一个用于自然语言处理的词向量处理的训练模型，而且还是只使用cupy库手搓出来的一个小型神经网络。然而接下来的事情让它倒吸了一口冷气：呼啦一下，那个程序从语料库中瞬间生成了一百多万个单词向量，组成了一个一百万乘一百的巨型矩阵。CPU下达了命令：它将这些数据全部传入了那张GTX 1650显卡中。一股数据洪流从GPU的一端汹涌而入，它能隐约分辨出组成洪流的分子，它们是一个个一维的词向量。除了那些3A游戏以外，GPU从未见过如此多的数据，它们占据了将近1GB的显存。这原始数据的洪流如炽热的岩浆，注入了网络每个Embedding层和采样层中，立刻一切都沸腾起来！GPU一千多个ALU进入了满负荷状态，温度飙升到近90摄氏度，优化器的梯度值在神经网络中浪涛汹涌，正向传播和反向传播的台风在呼啸，权重值亦随之此消彼长......这种状态持续了三十多分钟，这在GPU看来有几个世纪那样长。它终于松了一口气，它的能力用到极限，刚刚能控制这个疯狂的世界。台风弱下来，大洋也渐渐平静，又过了一会儿，台风消失了，大洋凝固，且急剧缩小，最后，它的精华凝结成一粒微小的数据种子，在内存无边的虚空中发出缕缕金光，这粒种子化作了2MB的训练权重数据文件。
